---
title: "HW4"
author: "Matthew Fein"
date: "February 17, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lab 6.1

##Data are relating to 13 variables and their relationship to housing prices in Boston

```{r cars}
library(ISLR)
library(MASS)
fix(Boston)
names(Boston)
dim(Boston)
sum(is.na(Boston))
library(leaps)
regfit.full=regsubsets(medv~.,Boston)
model.summary(regfit.full)
regfit.full=regsubsets(medv~.,data=Boston,nvmax=13)
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col="red",cex=2,pch=20)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary$cp)
points(11,reg.summary$cp[10],col="red",cex=2,pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(11,reg.summary$bic[6],col="red",cex=2,pch=20)
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
coef(regfit.full,11)



regfit.fwd=regsubsets(medv~.,data=Boston,nvmax=13,method="forward")
summary(regfit.fwd)
plot(regfit.fwd, scale = "Cp")
regfit.bwd=regsubsets(medv~.,data=Boston,nvmax=13,method="backward")
summary(regfit.bwd)
plot(regfit.bwd, scale = "Cp")
coef(regfit.full,11)
coef(regfit.fwd,11)
coef(regfit.bwd,11)
```
All of the different measures of model veracity show 11 included predictors as the optimal amount

```{r}
set.seed(1)
train=1:380
test=381:506
regfit.best=regsubsets(medv~.,data=Boston[train,],nvmax=13)
test.mat=model.matrix(medv~.,data=Boston[test,])
val.errors=rep(NA,13)
for(i in 1:13){
  coefi=coef(regfit.best,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((Boston$medv[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best,10)
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
regfit.best=regsubsets(medv~.,data=Boston,nvmax=13)
coef(regfit.best,10)
k=10
set.seed(1)
folds=sample(1:k,nrow(Boston),replace=TRUE)
cv.errors=matrix(NA,k,13, dimnames=list(NULL, paste(1:13)))
for(j in 1:k){
  best.fit=regsubsets(medv~.,data=Boston[folds!=j,],nvmax=13)
  for(i in 1:13){
    pred=predict(best.fit,Boston[folds==j,],id=i)
    cv.errors[j,i]=mean( (Boston$medv[folds==j]-pred)^2)
  }
}
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
reg.best=regsubsets(medv~.,data=Boston, nvmax=13)
coef(reg.best,11)
```
Further validation indicates 11 predictors as well.

## Lab 6.2



```{r pressure}
x=model.matrix(medv~.,Boston)[,-1]
y=Boston$medv

# Ridge Regression

library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
plot(ridge.mod, xvar = "lambda", label = TRUE)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod,s=50,type="coefficients")[1:14,]
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients",x=x[train,],y=y[train])[1:14,]
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:14,]

##The ridge shows that the optimal lambda is somewhere between 1 and 2 for this data
# The Lasso

lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:14,]
lasso.coef
lasso.coef[lasso.coef!=0]

##We use the lasso method and some coefficients are now restricted to 0. 
#We end up using shrinkage and feature selection.  It suggests 8 predictor variables.


```

##Lab 6.3

```{r}
# Principal Components Regression

library(pls)
set.seed(2)
pcr.fit=pcr(medv~., data=Boston,scale=TRUE,validation="CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP")
set.seed(1)
pcr.fit=pcr(medv~., data=Boston,subset=train,scale=TRUE, validation="CV")
validationplot(pcr.fit,val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)

# Partial Least Squares

set.seed(1)
pls.fit=plsr(medv~., data=Boston,subset=train,scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
pls.fit=plsr(medv~., data=Boston,scale=TRUE,ncomp=2)
summary(pls.fit)
```

There are 3 significant components looking at the scree plot.